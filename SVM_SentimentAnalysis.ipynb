{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916b21b3-09bd-4047-896f-879f7275e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b23cf5c3-0c45-4bb5-b131-6ed206d20a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VEERENDRA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VEERENDRA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\VEERENDRA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\VEERENDRA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\VEERENDRA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aff2b3ef-fbac-44b5-a536-531191ef8709",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "172eba7d-b58c-4c54-8d02-b71cc4ce8351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing function that handles various aspects of text cleaning\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): Input text to be preprocessed\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned and preprocessed text\n",
    "    \"\"\"\n",
    "    # Convert to string if not already\n",
    "    text = str(text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove hashtags and mentions\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Handle contractions (e.g., \"don't\" -> \"do not\")\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cff8cfc-daf9-4dde-bf8b-f9323cc4da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc18b0b1-784d-4369-aee9-3d37415473af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  category\n",
      "0  when modi promised “minimum government maximum...      -1.0\n",
      "1  talk all the nonsense and continue all the dra...       0.0\n",
      "2  what did just say vote for modi  welcome bjp t...       1.0\n",
      "3  asking his supporters prefix chowkidar their n...       1.0\n",
      "4  answer who among these the most powerful world...       1.0\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# # Take the first 10,000 rows\n",
    "# df = data.iloc[:10000]\n",
    "\n",
    "# # Display the subset data\n",
    "# print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d02076c6-a8de-4fe0-95fc-15ec2b7c58f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162980, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1ed7458-910a-494a-adfd-cfab7dc2affb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing texts...\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing texts...\")\n",
    "df['processed_text'] = df['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11efc0c8-e3f5-4dba-af9c-7f98829e9ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                      Text  category  \\\n",
       "0       when modi promised “minimum government maximum...      -1.0   \n",
       "1       talk all the nonsense and continue all the dra...       0.0   \n",
       "2       what did just say vote for modi  welcome bjp t...       1.0   \n",
       "3       asking his supporters prefix chowkidar their n...       1.0   \n",
       "4       answer who among these the most powerful world...       1.0   \n",
       "...                                                   ...       ...   \n",
       "162975  why these 456 crores paid neerav modi not reco...      -1.0   \n",
       "162976  dear rss terrorist payal gawar what about modi...      -1.0   \n",
       "162977  did you cover her interaction forum where she ...       0.0   \n",
       "162978  there big project came into india modi dream p...       0.0   \n",
       "162979  have you ever listen about like gurukul where ...       1.0   \n",
       "\n",
       "                                           processed_text  \n",
       "0       modi promised minimum government maximum gover...  \n",
       "1                  talk nonsense continue drama vote modi  \n",
       "2       say vote modi welcome bjp told rahul main camp...  \n",
       "3       asking supporter prefix chowkidar name modi gr...  \n",
       "4       answer among powerful world leader today trump...  \n",
       "...                                                   ...  \n",
       "162975  crore paid neerav modi recovered congress lead...  \n",
       "162976  dear rss terrorist payal gawar modi killing pl...  \n",
       "162977                       cover interaction forum left  \n",
       "162978  big project came india modi dream project happ...  \n",
       "162979  ever listen like gurukul discipline maintained...  \n",
       "\n",
       "[162980 rows x 3 columns]>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a070a7d-acfb-4d26-b261-2c53d46276bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text              8699\n",
       "category          8698\n",
       "processed_text       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_count = df.isna().sum()\n",
    "nan_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e02d02e8-752d-48c0-85e2-e555d85a5940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52b8b216-cb2b-4099-9c02-aada20593a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['processed_text'], \n",
    "    df['category'],\n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df['category']  # Ensure balanced split\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada1ea54-02a0-4d49-9db7-d89d59e12e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c9eda-5a0c-4acb-845c-dda2f6ddad3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f1746-170d-4a3a-82c3-470f86eb1c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae909131-9406-45a5-8767-8a1dcd60188d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF vectors...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating TF-IDF vectors...\")\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,  # Limit features to prevent overfitting\n",
    "    ngram_range=(1, 2),  # Use both unigrams and bigrams\n",
    "    min_df=5  # Minimum document frequency\n",
    ")\n",
    "\n",
    "# Transform the text data into TF-IDF features\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fa4f4a7-272c-4bef-bdc2-50c56c34821f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;ovr&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;ovr&#x27;, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, multi_class='ovr', random_state=42)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = LogisticRegression(\n",
    "#     multi_class='ovr',  # One-vs-Rest strategy\n",
    "#     max_iter=1000,      # Increase max iterations to ensure convergence\n",
    "#     C=1.0,             # Inverse of regularization strength\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Fit the model\n",
    "# model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a965827-48a5-4cc9-8813-0a0930d4fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "852c6fb0-7b1c-48b8-9b0c-af8059a99bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_svm(kernel='linear'):\n",
    "    print(f\"\\nTraining SVM with {kernel} kernel...\")\n",
    "    \n",
    "    # Initialize SVM\n",
    "    svm_model = SVC(\n",
    "        kernel=kernel,\n",
    "        C=1.0,\n",
    "        random_state=42,\n",
    "        probability=True\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    svm_model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = svm_model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\nClassification Report for {kernel} kernel SVM:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return svm_model, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31bfca49-11f9-4bee-aa58-f2e930f32d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Linear Kernel SVM ===\n",
      "\n",
      "Training SVM with linear kernel...\n",
      "\n",
      "Classification Report for linear kernel SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.85      0.77      0.81      6746\n",
      "         0.0       0.84      0.96      0.90     10464\n",
      "         1.0       0.93      0.87      0.90     13646\n",
      "\n",
      "    accuracy                           0.88     30856\n",
      "   macro avg       0.87      0.87      0.87     30856\n",
      "weighted avg       0.88      0.88      0.88     30856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Linear Kernel SVM ===\")\n",
    "linear_svm, linear_predictions = train_evaluate_svm('linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3af82635-7b22-4729-95c8-65b618fc60da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved to linear_svm_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "model_filename = \"linear_svm_model.pkl\"\n",
    "with open(model_filename, \"wb\") as file:\n",
    "    pickle.dump(linear_svm, file)\n",
    "\n",
    "print(f\"\\nModel saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be18e5f-37e7-49f1-9ba1-a803d934a469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
