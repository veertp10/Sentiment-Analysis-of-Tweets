{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10140054,"sourceType":"datasetVersion","datasetId":6258367}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:36:37.546067Z","iopub.execute_input":"2024-12-08T15:36:37.546347Z","iopub.status.idle":"2024-12-08T15:36:39.469242Z","shell.execute_reply.started":"2024-12-08T15:36:37.546318Z","shell.execute_reply":"2024-12-08T15:36:39.468390Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/bilstm-sentiment-analysis/task3/train3.csv\n/kaggle/input/bilstm-sentiment-analysis/task3/test3.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport tensorflow as tf\nimport re\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:38:17.996342Z","iopub.execute_input":"2024-12-08T15:38:17.996880Z","iopub.status.idle":"2024-12-08T15:38:18.002376Z","shell.execute_reply.started":"2024-12-08T15:38:17.996848Z","shell.execute_reply":"2024-12-08T15:38:18.001242Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Kaggle optimizations\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\nos.environ['TORCH_DISTRIBUTED_DEBUG'] = 'INFO'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:36:39.470575Z","iopub.execute_input":"2024-12-08T15:36:39.470958Z","iopub.status.idle":"2024-12-08T15:36:39.475078Z","shell.execute_reply.started":"2024-12-08T15:36:39.470926Z","shell.execute_reply":"2024-12-08T15:36:39.474131Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Check GPU availability and type\nimport torch\nprint(\"CUDA Available:\", torch.cuda.is_available())\nprint(\"GPU Device:\", torch.cuda.get_device_name(0))\nprint(\"Number of GPUs:\", torch.cuda.device_count())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:36:39.475960Z","iopub.execute_input":"2024-12-08T15:36:39.476325Z","iopub.status.idle":"2024-12-08T15:36:44.821548Z","shell.execute_reply.started":"2024-12-08T15:36:39.476284Z","shell.execute_reply":"2024-12-08T15:36:44.820671Z"}},"outputs":[{"name":"stdout","text":"CUDA Available: True\nGPU Device: Tesla T4\nNumber of GPUs: 2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Load and preprocess data\ndf = pd.read_csv('/kaggle/input/bilstm-sentiment-analysis/task3/train3.csv')\n\n# Remove rows with NaN values\ndf = df.dropna()\n\n# Convert labels to numeric format (-1 -> 0, 0 -> 1, 1 -> 2)\nlabel_map = {-1: 0, 0: 1, 1: 2}\ndf['category'] = df['category'].map(label_map)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:37:07.793119Z","iopub.execute_input":"2024-12-08T15:37:07.793476Z","iopub.status.idle":"2024-12-08T15:37:08.457394Z","shell.execute_reply.started":"2024-12-08T15:37:07.793446Z","shell.execute_reply":"2024-12-08T15:37:08.456641Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Text preprocessing\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = str(text).lower()\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    # Remove user mentions\n    text = re.sub(r'@\\w+', '', text)\n    # Remove hashtags\n    text = re.sub(r'#\\w+', '', text)\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# Apply preprocessing\ndf['Text'] = df['Text'].apply(preprocess_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:39:04.113304Z","iopub.execute_input":"2024-12-08T15:39:04.114062Z","iopub.status.idle":"2024-12-08T15:39:06.311546Z","shell.execute_reply.started":"2024-12-08T15:39:04.114029Z","shell.execute_reply":"2024-12-08T15:39:06.310796Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Prepare data for model\ntexts = df['Text'].values\nlabels = df['category'].values\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:40:01.826046Z","iopub.execute_input":"2024-12-08T15:40:01.826375Z","iopub.status.idle":"2024-12-08T15:40:01.850580Z","shell.execute_reply.started":"2024-12-08T15:40:01.826350Z","shell.execute_reply":"2024-12-08T15:40:01.849949Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Tokenization\nmax_words = 10000  # Maximum number of words to keep\nmax_len = 100      # Maximum length of each sequence\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:40:16.095884Z","iopub.execute_input":"2024-12-08T15:40:16.096249Z","iopub.status.idle":"2024-12-08T15:40:21.006481Z","shell.execute_reply.started":"2024-12-08T15:40:16.096222Z","shell.execute_reply":"2024-12-08T15:40:21.005742Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Pad sequences\nX_train = pad_sequences(X_train, maxlen=max_len)\nX_test = pad_sequences(X_test, maxlen=max_len)\n# Convert labels to categorical\ny_train = tf.keras.utils.to_categorical(y_train)\ny_test = tf.keras.utils.to_categorical(y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:40:42.456400Z","iopub.execute_input":"2024-12-08T15:40:42.456746Z","iopub.status.idle":"2024-12-08T15:40:42.755242Z","shell.execute_reply.started":"2024-12-08T15:40:42.456718Z","shell.execute_reply":"2024-12-08T15:40:42.754404Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Build BiLSTM model\nmodel = Sequential([\n    Embedding(max_words, 128, input_length=max_len),\n    Bidirectional(LSTM(64, return_sequences=True)),\n    Bidirectional(LSTM(32)),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(3, activation='softmax')  # 3 classes: negative, neutral, positive\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:40:56.105818Z","iopub.execute_input":"2024-12-08T15:40:56.106176Z","iopub.status.idle":"2024-12-08T15:40:56.908631Z","shell.execute_reply.started":"2024-12-08T15:40:56.106147Z","shell.execute_reply":"2024-12-08T15:40:56.907868Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Compile model\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Add early stopping\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    restore_best_weights=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:41:29.906187Z","iopub.execute_input":"2024-12-08T15:41:29.907169Z","iopub.status.idle":"2024-12-08T15:41:29.936438Z","shell.execute_reply.started":"2024-12-08T15:41:29.907119Z","shell.execute_reply":"2024-12-08T15:41:29.935455Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"with tf.device('/GPU:0'):\n    history = model.fit(\n        X_train, y_train,\n        epochs=10,\n        batch_size=64,\n        validation_split=0.2,\n        callbacks=[early_stopping],\n        verbose=1\n    )\n\n# Evaluate model\ntest_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(f\"\\nTest accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:41:45.843851Z","iopub.execute_input":"2024-12-08T15:41:45.844506Z","iopub.status.idle":"2024-12-08T15:49:14.715366Z","shell.execute_reply.started":"2024-12-08T15:41:45.844474Z","shell.execute_reply":"2024-12-08T15:49:14.714565Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m1543/1543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 50ms/step - accuracy: 0.7800 - loss: 0.5277 - val_accuracy: 0.9626 - val_loss: 0.1293\nEpoch 2/10\n\u001b[1m1543/1543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 45ms/step - accuracy: 0.9676 - loss: 0.1191 - val_accuracy: 0.9687 - val_loss: 0.1136\nEpoch 3/10\n\u001b[1m1543/1543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 45ms/step - accuracy: 0.9756 - loss: 0.0924 - val_accuracy: 0.9708 - val_loss: 0.1101\nEpoch 4/10\n\u001b[1m1543/1543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 45ms/step - accuracy: 0.9814 - loss: 0.0680 - val_accuracy: 0.9710 - val_loss: 0.1119\nEpoch 5/10\n\u001b[1m1543/1543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 45ms/step - accuracy: 0.9856 - loss: 0.0516 - val_accuracy: 0.9701 - val_loss: 0.1300\nEpoch 6/10\n\u001b[1m1543/1543\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 45ms/step - accuracy: 0.9891 - loss: 0.0386 - val_accuracy: 0.9644 - val_loss: 0.1519\n\u001b[1m965/965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.9694 - loss: 0.1204\n\nTest accuracy: 0.9693\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def predict_sentiment(text):\n    # Preprocess the text\n    processed_text = preprocess_text(text)\n    # Convert to sequence\n    sequence = tokenizer.texts_to_sequences([processed_text])\n    # Pad sequence\n    padded = pad_sequences(sequence, maxlen=max_len)\n    # Predict\n    prediction = model.predict(padded)\n    # Get class with highest probability\n    sentiment_class = np.argmax(prediction)\n    # Map back to original labels\n    sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n    return sentiment_map[sentiment_class]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:50:39.321071Z","iopub.execute_input":"2024-12-08T15:50:39.321477Z","iopub.status.idle":"2024-12-08T15:50:39.326916Z","shell.execute_reply.started":"2024-12-08T15:50:39.321446Z","shell.execute_reply":"2024-12-08T15:50:39.326108Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Test the model with some example tweets\nexample_tweets = [\n    \"when modi promised minimum government maximum governance\",\n    \"talk all the nonsense and continue all the drama\",\n    \"what did just say vote for modi welcome bjp\"\n]\n\nfor tweet in example_tweets:\n    sentiment = predict_sentiment(tweet)\n    print(f\"\\nTweet: {tweet}\")\n    print(f\"Predicted sentiment: {sentiment}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:50:43.833660Z","iopub.execute_input":"2024-12-08T15:50:43.834059Z","iopub.status.idle":"2024-12-08T15:50:44.440853Z","shell.execute_reply.started":"2024-12-08T15:50:43.834025Z","shell.execute_reply":"2024-12-08T15:50:44.440092Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step\n\nTweet: when modi promised minimum government maximum governance\nPredicted sentiment: neutral\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n\nTweet: talk all the nonsense and continue all the drama\nPredicted sentiment: neutral\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n\nTweet: what did just say vote for modi welcome bjp\nPredicted sentiment: positive\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}