{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  pip subprocess to install build dependencies did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [95 lines of output]\n",
      "  Ignoring numpy: markers 'python_version >= \"3.9\"' don't match your environment\n",
      "  Collecting setuptools\n",
      "    Using cached setuptools-75.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Collecting cython<3.0,>=0.25\n",
      "    Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "  Collecting cymem<2.1.0,>=2.0.2\n",
      "    Using cached cymem-2.0.10.tar.gz (10 kB)\n",
      "    Installing build dependencies: started\n",
      "    Installing build dependencies: finished with status 'done'\n",
      "    Getting requirements to build wheel: started\n",
      "    Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing metadata (pyproject.toml): started\n",
      "    Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "  Collecting preshed<3.1.0,>=3.0.2\n",
      "    Using cached preshed-3.0.9-cp38-cp38-win_amd64.whl.metadata (2.2 kB)\n",
      "  Collecting murmurhash<1.1.0,>=0.28.0\n",
      "    Using cached murmurhash-1.0.11.tar.gz (13 kB)\n",
      "    Installing build dependencies: started\n",
      "    Installing build dependencies: finished with status 'done'\n",
      "    Getting requirements to build wheel: started\n",
      "    Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing metadata (pyproject.toml): started\n",
      "    Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "  Collecting thinc<8.4.0,>=8.3.0\n",
      "    Using cached thinc-8.3.2.tar.gz (193 kB)\n",
      "    Installing build dependencies: started\n",
      "    Installing build dependencies: finished with status 'error'\n",
      "    error: subprocess-exited-with-error\n",
      "  \n",
      "    pip subprocess to install build dependencies did not run successfully.\n",
      "    exit code: 1\n",
      "  \n",
      "    [51 lines of output]\n",
      "    Ignoring numpy: markers 'python_version >= \"3.9\"' don't match your environment\n",
      "    Collecting setuptools\n",
      "      Using cached setuptools-75.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "    Collecting cython<3.0,>=0.25\n",
      "      Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "    Collecting murmurhash<1.1.0,>=1.0.2\n",
      "      Using cached murmurhash-1.0.11.tar.gz (13 kB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'done'\n",
      "      Getting requirements to build wheel: started\n",
      "      Getting requirements to build wheel: finished with status 'done'\n",
      "      Preparing metadata (pyproject.toml): started\n",
      "      Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "    Collecting cymem<2.1.0,>=2.0.2\n",
      "      Using cached cymem-2.0.10.tar.gz (10 kB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'done'\n",
      "      Getting requirements to build wheel: started\n",
      "      Getting requirements to build wheel: finished with status 'done'\n",
      "      Preparing metadata (pyproject.toml): started\n",
      "      Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "    Collecting preshed<3.1.0,>=3.0.2\n",
      "      Using cached preshed-3.0.9-cp38-cp38-win_amd64.whl.metadata (2.2 kB)\n",
      "    Collecting blis<1.1.0,>=1.0.0\n",
      "      Using cached blis-1.0.1.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      error: subprocess-exited-with-error\n",
      "  \n",
      "      pip subprocess to install build dependencies did not run successfully.\n",
      "      exit code: 1\n",
      "  \n",
      "      [7 lines of output]\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached Cython-3.0.11-cp38-cp38-win_amd64.whl.metadata (3.2 kB)\n",
      "      ERROR: Ignored the following versions that require a different python version: 1.25.0 Requires-Python >=3.9; 1.25.1 Requires-Python >=3.9; 1.25.2 Requires-Python >=3.9; 1.26.0 Requires-Python <3.13,>=3.9; 1.26.1 Requires-Python <3.13,>=3.9; 1.26.2 Requires-Python >=3.9; 1.26.3 Requires-Python >=3.9; 1.26.4 Requires-Python >=3.9; 2.0.0 Requires-Python >=3.9; 2.0.1 Requires-Python >=3.9; 2.0.2 Requires-Python >=3.9; 2.1.0 Requires-Python >=3.10; 2.1.0rc1 Requires-Python >=3.10; 2.1.1 Requires-Python >=3.10; 2.1.2 Requires-Python >=3.10; 2.1.3 Requires-Python >=3.10; 2.2.0rc1 Requires-Python >=3.10; 75.4.0 Requires-Python >=3.9; 75.5.0 Requires-Python >=3.9; 75.6.0 Requires-Python >=3.9\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      [end of output]\n",
      "  \n",
      "      note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "    error: subprocess-exited-with-error\n",
      "  \n",
      "    pip subprocess to install build dependencies did not run successfully.\n",
      "    exit code: 1\n",
      "  \n",
      "    See above for output.\n",
      "  \n",
      "    note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "    [end of output]\n",
      "  \n",
      "    note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  pip subprocess to install build dependencies did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  See above for output.\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "pip subprocess to install build dependencies did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached spacy-3.8.2.tar.gz (1.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# Import additional libraries for analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import contractions\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style for visualizations\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('/kaggle/input/sentiments/task3/train3.csv')\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Number of tweets: {len(df)}\")\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download all required resources explicitly\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Open Multilingual WordNet\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Then initialize your lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = df.isna().sum()\n",
    "nan_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_preprocess_text(text):\n",
    "    \"\"\"\n",
    "    A simpler preprocessing function that doesn't rely on WordNet\n",
    "    \"\"\"\n",
    "    # Convert to string if not already\n",
    "    text = str(text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove hashtags and mentions\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Handle contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Simple tokenization by splitting on whitespace\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Try the simpler preprocessing if needed\n",
    "df['processed_text'] = df['Text'].apply(simple_preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Basic Dataset Analysis\n",
    "print(\"\\n1. Distribution of Sentiments\")\n",
    "sentiment_dist = df['category'].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=sentiment_dist.index, y=sentiment_dist.values)\n",
    "plt.title('Distribution of Sentiments')\n",
    "plt.xlabel('category')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "dft = pd.read_csv('/kaggle/input/sentiments/task3/test3.csv')\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Number of tweets: {len(df)}\")\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class imbalance\n",
    "print(\"\\nClass Distribution Percentages:\")\n",
    "print(df['category'].value_counts(normalize=True) * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Text Length Analysis\n",
    "df['text_length'] = df['Text'].str.len()\n",
    "df['word_count'] = df['Text'].str.split().str.len()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Text length distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(x='category', y='text_length', data=df)\n",
    "plt.title('Text Length by Sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Text Length')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Word count distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='category', y='word_count', data=df)\n",
    "plt.title('Word Count by Sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Word Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nText Length Statistics:\")\n",
    "print(df.groupby('category')['text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Distribution Analysis\n",
    "def analyze_word_distributions(text_series, category_series):\n",
    "    \"\"\"\n",
    "    Analyzes word usage patterns across different categories to identify distinctive\n",
    "    language patterns that might help in sentiment classification.\n",
    "    \"\"\"\n",
    "    print(\"Analyzing word distributions across categories...\")\n",
    "    \n",
    "    # Create word frequency distributions for each category\n",
    "    category_word_dist = {}\n",
    "    for category in category_series.unique():\n",
    "        # Get texts for this category\n",
    "        category_texts = text_series[category_series == category]\n",
    "        \n",
    "        # Combine all texts and clean\n",
    "        combined_text = ' '.join(category_texts).lower()\n",
    "        # Remove URLs, mentions, special characters\n",
    "        cleaned_text = re.sub(r'http\\S+|www\\S+|@\\w+|#\\w+|[^a-zA-Z\\s]', '', combined_text)\n",
    "        # Tokenize and remove stopwords\n",
    "        words = [word for word in word_tokenize(cleaned_text) \n",
    "                if word not in stop_words and len(word) > 2]\n",
    "        \n",
    "        # Store word frequencies\n",
    "        category_word_dist[category] = Counter(words)\n",
    "        \n",
    "        # Plot top words for this category\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        common_words = dict(Counter(words).most_common(15))\n",
    "        plt.bar(common_words.keys(), common_words.values())\n",
    "        plt.title(f'Most Common Words in Category: {category}')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nTop 15 words in category {category}:\")\n",
    "        for word, count in Counter(words).most_common(15):\n",
    "            print(f\"{word}: {count}\")\n",
    "    \n",
    "    return category_word_dist\n",
    "\n",
    "# Perform word distribution analysis\n",
    "word_distributions = analyze_word_distributions(df['Text'], df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing Distinctive Words\n",
    "def analyze_distinctive_words(word_distributions):\n",
    "    \"\"\"\n",
    "    Identifies words that are strongly associated with particular categories,\n",
    "    which can be valuable features for classification.\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing distinctive words for each category...\")\n",
    "    \n",
    "    # Calculate word importance scores using TF-IDF-like approach\n",
    "    total_categories = len(word_distributions)\n",
    "    distinctive_words = {}\n",
    "    \n",
    "    for category, word_dist in word_distributions.items():\n",
    "        # Calculate distinctiveness score for each word\n",
    "        word_scores = {}\n",
    "        for word, count in word_dist.items():\n",
    "            # Count in how many other categories this word appears\n",
    "            category_presence = sum(1 for other_dist in word_distributions.values()\n",
    "                                 if word in other_dist)\n",
    "            \n",
    "            # Calculate distinctiveness score (similar to TF-IDF)\n",
    "            distinctiveness = count * np.log(total_categories / category_presence)\n",
    "            word_scores[word] = distinctiveness\n",
    "        \n",
    "        # Store top distinctive words\n",
    "        distinctive_words[category] = dict(sorted(word_scores.items(), \n",
    "                                                key=lambda x: x[1], \n",
    "                                                reverse=True)[:10])\n",
    "        \n",
    "        print(f\"\\nMost distinctive words for category {category}:\")\n",
    "        for word, score in distinctive_words[category].items():\n",
    "            print(f\"{word}: {score:.2f}\")\n",
    "\n",
    "# Analyze distinctive words\n",
    "analyze_distinctive_words(word_distributions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Analysis\n",
    "def analyze_bigrams(text_series, category_series):\n",
    "    \"\"\"\n",
    "    Analyzes common word pairs to understand contextual patterns in language\n",
    "    usage across categories.\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing bigram patterns...\")\n",
    "    \n",
    "    for category in category_series.unique():\n",
    "        # Get texts for this category\n",
    "        category_texts = text_series[category_series == category]\n",
    "        \n",
    "        # Combine and clean texts\n",
    "        combined_text = ' '.join(category_texts).lower()\n",
    "        cleaned_text = re.sub(r'http\\S+|www\\S+|@\\w+|#\\w+|[^a-zA-Z\\s]', '', combined_text)\n",
    "        \n",
    "        # Generate bigrams\n",
    "        tokens = [word for word in word_tokenize(cleaned_text) \n",
    "                 if word not in stop_words and len(word) > 2]\n",
    "        bigram_freq = Counter(ngrams(tokens, 2))\n",
    "        \n",
    "        # Plot top bigrams\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        common_bigrams = dict(bigram_freq.most_common(10))\n",
    "        plt.bar([' '.join(bg) for bg in common_bigrams.keys()], \n",
    "                common_bigrams.values())\n",
    "        plt.title(f'Most Common Bigrams in Category: {category}')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nTop 10 bigrams in category {category}:\")\n",
    "        for bigram, count in bigram_freq.most_common(10):\n",
    "            print(f\"{' '.join(bigram)}: {count}\")\n",
    "\n",
    "# Perform bigram analysis\n",
    "analyze_bigrams(df['Text'], df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Length Analysis\n",
    "def analyze_text_length_patterns(text_series, category_series):\n",
    "    \"\"\"\n",
    "    Examines how text length varies across categories, which can reveal\n",
    "    relationships between message length and sentiment.\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing text length patterns...\")\n",
    "    \n",
    "    # Calculate text lengths\n",
    "    text_lengths = text_series.str.len()\n",
    "    word_counts = text_series.str.split().str.len()\n",
    "    \n",
    "    # Create length distributions plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x=category_series, y=text_lengths)\n",
    "    plt.title('Text Length Distribution by Category')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Number of Characters')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistical summary\n",
    "    print(\"\\nText length statistics by category:\")\n",
    "    length_stats = pd.DataFrame({\n",
    "        'text_length': text_lengths,\n",
    "        'word_count': word_counts,\n",
    "        'category': category_series\n",
    "    }).groupby('category').describe()\n",
    "    print(length_stats)\n",
    "\n",
    "# Analyze text length patterns\n",
    "analyze_text_length_patterns(df['Text'], df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['processed_text'], \n",
    "    df['category'],\n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df['category']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectors\n",
    "print(\"Creating TF-IDF vectors...\")\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,  # Limit features to prevent overfitting\n",
    "    ngram_range=(1, 2), # Use both unigrams and bigrams\n",
    "    min_df=5            # Minimum document frequency\n",
    ")\n",
    "\n",
    "# Transform the text data into TF-IDF features\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_evaluate_svm(kernel='linear'):\n",
    "    print(f\"\\nTraining SVM with {kernel} kernel...\")\n",
    "    \n",
    "    # Initialize SVM\n",
    "    svm_model = SVC(\n",
    "        kernel=kernel,\n",
    "        C=1.0,\n",
    "        random_state=42,\n",
    "        probability=True\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    svm_model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = svm_model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\nClassification Report for {kernel} kernel SVM:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return svm_model, y_pred\n",
    "\n",
    "# Train and evaluate both Linear and RBF kernel SVMs\n",
    "print(\"\\n=== Linear Kernel SVM ===\")\n",
    "linear_svm, linear_predictions = train_evaluate_svm('linear')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search(kernel='linear'):\n",
    "    print(f\"\\nPerforming Grid Search for {kernel} kernel...\")\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto'] if kernel == 'rbf' else ['scale']\n",
    "    }\n",
    "    \n",
    "    # Initialize SVM\n",
    "    svm = SVC(kernel=kernel, random_state=42)\n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        svm,\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit grid search\n",
    "    grid_search.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save predictions to CSV\n",
    "def save_predictions(predictions, filename):\n",
    "    test_predictions = pd.DataFrame({\n",
    "        'text': X_test,\n",
    "        'processed_text': X_test,\n",
    "        'predicted_sentiment': predictions\n",
    "    })\n",
    "    test_predictions.to_csv(filename, index=False)\n",
    "    print(f\"\\nPredictions saved to {filename}\")\n",
    "\n",
    "# Save predictions for both models\n",
    "save_predictions(linear_predictions, 'test3_linear_svm.csv')\n",
    "save_predictions(rbf_predictions, 'test3_rbf_svm.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6255164,
     "sourceId": 10135374,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6255229,
     "sourceId": 10135464,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
